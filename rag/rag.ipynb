{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Speech is a human vocal communication using language. Each language uses phonetic combinations of vowel and consonant sounds that form the sound of its words (that is, all English words sound different from all French words, even if they are the same word, e.g., \"role\" or \"hotel\"), and using those words in their semantic character as words in the lexicon of a language according to the syntactic constraints that govern lexical words\\' function in a sentence. In speaking, speakers perform many different intentional speech acts, e.g., informing, declaring, asking, persuading, directing, and can use enunciation, intonation, degrees of loudness, tempo, and other non-representational or paralinguistic aspects of vocalization to convey meaning. In their speech, speakers also unintentionally communicate many aspects of their social position such as sex, age, place of origin (through accent), physical states (alertness and sleepiness, vigor or weakness, health or illness), psychological states (emotions or moods), physico-psychological states (sobriety or drunkenness, normal consciousness and trance states), education or experience, and the like.\\n\\nAlthough people ordinarily use speech in dealing with other persons (or animals), when people swear they do not always mean to communicate anything to anyone, and sometimes in expressing urgent emotions or desires they use speech as a quasi-magical cause, as when they encourage a player in a game to do or warn them not to do something. There are also many situations in which people engage in solitary speech. People talk to themselves sometimes in acts that are a development of what some psychologists (e.g., Lev Vygotsky) have maintained is the use of silent speech in an interior monologue to vivify and organize cognition, sometimes in the momentary adoption of a dual persona as self addressing self as though addressing another person. Solo speech can be used to memorize or to test one\\'s memorization of things, and in prayer or in meditation (e.g., the use of a mantra).\\n\\nResearchers study many different aspects of speech: speech production and speech perception of the sounds used in a language, speech repetition, speech errors, the ability to map heard spoken words onto the vocalizations needed to recreate them, which plays a key role in children\\'s enlargement of their vocabulary, and what different areas of the human brain, such as Broca\\'s area and Wernicke\\'s area, underlie speech. Speech is the subject of study for linguistics, cognitive science, communication studies, psychology, computer science, speech pathology, otolaryngology, and acoustics. Speech compares with written language,[1] which may differ in its vocabulary, syntax, and phonetics from the spoken language, a situation called diglossia.    ', metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "speech_text = loader.load()\n",
    "speech_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\nFinding the Words to Say: Hidden State Visualizations for Language Models – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.@JayAlammar on Twitter. YouTube Channel\\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nFinding the Words to Say: Hidden State Visualizations for Language Models\\n\\n\\n\\n\\n\\nBy visualizing the hidden state between a model\\'s layers, we can get some clues as to the model\\'s \"thought process\".\\n\\n\\n\\n\\n\\nFigure: Finding the words to say\\n            After a language model generates a sentence, we can visualize a view of how the model came by each word (column).  Each row is a model layer. The value and color indicate the ranking of the output token at that layer. The darker the color, the higher the ranking. Layer 0 is at the top. Layer 47 is at the bottom.\\nModel:GPT2-XL\\n\\n\\n\\n\\nPart 2: Continuing the pursuit of making Transformer language models more transparent, this article showcases a collection of visualizations to uncover mechanics of language generation inside a pre-trained language model. These visualizations are all created using Ecco, the open-source package we\\'re releasing\\n\\nIn the first part of this series, Interfaces for Explaining Transformer Language Models, we showcased interactive interfaces for input saliency and neuron activations. In this article, we will focus on the hidden state as it evolves from model layer to the next. By looking at the hidden states produced by every transformer decoder block, we aim to gleam information about how a language model arrived at a specific output token. This method is explored by Voita et al.. Nostalgebraist \\n        presents compelling visual treatments showcasing the evolution of token rankings, logit scores, and softmax\\n        probabilities for the evolving hidden state through the various layers of the model.\\n    \\n\\nRecap: Transformer Hidden States\\nThe following figure recaps how a transformer language model works. How the layers result in a final hidden state. And how that final state is then projected to the output vocabulary which results in a score assigned to each token in\\n        the model\\'s vocabulary. We can see here the top scoring tokens when DistilGPT2 is fed the input sequence \" 1, 1,\\n        \":\\n\\n\\n\\nFigure: Recap of transformer language models.\\n            This figure shows how the model arrives at the top five output token candidates and their probability scores. This shows us that at the final layer, the\\n            model is 59% sure the next token is \\' 1\\', and that would be chosen as the output token by greedy decoding.\\n            Other probable outputs include \\' 2\\' with 18% probability (maybe we are counting) and \\' 0\\' with 5%\\n            probability (maybe we are counting down).\\n        \\n\\n\\nEcco provides a view of the model\\'s top scoring tokens and their probability scores.\\n\\n\\n# Generate one token to complete this input string\\noutput = lm.generate(\" 1, 1, 1,\", generate=1)\\n\\n# Visualize\\noutput.layer_predictions(position=6, layer=5)\\n\\n\\nWhich would show the following breakdown of candidate output tokens and their probability scores:\\n\\n\\n\\n\\nFigure: Ten tokens with highest probabilities at the final layer of the model.\\n\\n\\nScores after each layer\\nApplying the same projection to internal hidden states of the model gives us a view of how the model\\'s conviction\\n        for the output scoring developed over the processing of the inputs. This projection of internal hidden states\\n        gives us a sense of which layer contributed the most to elevating the scores (and hence ranking) of a certain\\n        potential output token.\\n\\n\\n\\nFigure: projecting inner hidden states to the model\\'s vocabulary reveals cues of processing between layers.\\n\\n\\nViewing the evolution of the hidden states means that instead of looking only at the candidates output tokens from\\n        projecting the final model state, we can look at the top scoring tokens after projecting the hidden state\\n        resulting from each of the model\\'s six layers.\\n\\n\\nThis visualization is created using the same method above with omitting the \\'layer\\' argument (which we set to the final layer in the previous example, layer #5):\\n\\n# Visualize the top scoring tokens after each layer\\noutput.layer_predictions(position=6)\\n\\n\\nResulting in: \\n\\n\\n\\n\\nFigure: Top scoring tokens after each of the model\\'s six layers.\\n\\n            Each row shows the top ten predicted tokens obtained by projecting each hidden state to the output\\n            vocabulary. The probability scores are shown in pink (obtained by passing logit scores through softmax). We\\n            can see that Layer 0 has no digits in its top ten predictions. Layer 1\\n            gives the token \\' 1\\' a 0.03%, probability which, while low, still ranks the token as the seventh highest\\n            ranking token. Subsequent layers keep elevating the probability and ranking of \\' 1\\', until the final\\n            layer injects a bit more caution by reducing the probability from 100% to ~60%, still retaining the\\n            token as the highest ranked in the model\\'s output.\\nNote: This figure is incorrect in showing 0 probability assigned to some tokens due to rounding. The current version of Ecco fixes this by showing \\'<0.01%\\'.\\n        \\n\\n\\n\\n\\n\\nYou can experiment with these visualizations and experiment with them on your own input sentences at the following colab link:\\n\\n\\n\\nEvolution of the selected token\\n\\n\\n\\nThe ranking of the token \\' 1\\' after each layer\\nLayer 0 elevated the token \\' 1\\' to be the 31st highest scored token in the hidden state it\\n            produced. Layers 1 and 2 kept increasing the ranking (to 7 then 5 respectively). All the\\n            following layers were sure this is the best token and gave it the top ranking spot.\\n        \\n\\nAnother visual perspective on the evolving hidden states is to re-examine the hidden states after selecting an output\\n        token to see how the hidden state after each layer ranked that token. This is one of the many perspectives\\n        explored by Nostalgebraist \\n        and the one we think is a great first approach. In the figure on the side, we can see the ranking (out of\\n        +50,0000 tokens in the model\\'s vocabulary) of the token \\' 1\\' where each row\\n        indicates a layer\\'s output.\\n    \\nThe same visualization can then be plotted for an entire generated sequence, where each column indicates a\\n        generation step (and its output token), and each row the ranking of\\n        the output token at each layer:\\n    \\n\\n\\n\\nEvolution of the rankings of the output sequence \\' 1 , 1\\'\\n            We can see that Layer 3 is the point at which the model started to be certain of\\n            the digit \\' 1\\' as the output. When the output is to be a comma, Layer 0 usually ranks\\n            the comma as 5. \\nWhen the output is to be a \\' 1\\', Layer 0 is less certain, but still ranks the \\' 1\\' token at\\n            31 or 32.\\n            Notice that every output token is ranked #1 after Layer 5. That is the definition of greedy\\n            sampling -- the reason we selected this token is because it was ranked first.\\n        \\n\\nLet us demonstrate this visualization by presenting the following input to GPT2-Large:\\n\\n\\ninput:0The1 countries2 of3 the4 European5 Union6 are7:8\\\\n9110.11 Austria12\\\\n13214.15 Belgium16\\\\n17318.19 Bulgaria20\\\\n21422.\\n\\n\\nVisualizaing the evolution of the hidden states sheds light on how various layers contribute to generating this sequence as we can see in the following figure:\\n\\n\\n\\nFigure: Hidden state evolution of an output sequence \\n            Click to open image in full resolution. The figure reveals:\\n            \\nColumns of solid pink corresponding to newlines and periods. Starting from Layer #0 and onwards, the model is certain early on of these tokens, indicating Layer #0\\'s awareness of certain syntactic properties (and that later layers raise no objections).\\n\\n                    Columns where country names are predicted are very bright at the top and it\\'s up to the last five layers to really come up with the appropriate token.\\n                \\n\\n                    Columns tracking the incrementing number tend to be resolved at layer #9.\\n                \\n\\n                    The model erroneously lists Chile in the list, not a EU country. But notice that the ranking of that token is 43 -- indicating the error is better attributed to our token sampling method rather than to the model itself. In the case of all other countries they were correct and in the top 3.\\n                \\n\\n                    Aside from Chile, the rest of the countries are correct, but also follow the alphabetical order followed in the input sequence.\\n                \\n\\n\\n\\n\\nRankings of Other Tokens\\n\\n\\n\\nFigure: Rankings of which token should go in the blank \\n            While the final output succeeds in assigning the correct number, the first five layers surprisingly fail at\\n            identifying the correct number (by giving \" is\" a higher ranking than \" are\", which is the correct answer).\\n            Examining attention or inner-layer saliency could reveal clues as to the reason.\\n        \\n\\nWe are not limited to watching the evolution of only one (the selected) token for a specific position. There are\\n        cases where we want to compare the rankings of multiple tokens in the same position regardless if the model selected them or not. \\nOne such case is the number prediction task described by Linzen et al.\\n        which arises from the English language phenomenon of subject-verb agreement. In that task, we want to analyze the\\n        model\\'s capacity to encode syntactic number (whether the subject we\\'re addressing is singular or plural)\\n        and syntactic subjecthood (which subject in the sentence we\\'re addressing).\\n    \\nPut simply, fill-in the blank. The only acceptable answers are 1) is 2) are:\\n    \\nThe keys to the cabinet ______ \\nTo answer correctly, one has to first determine whether we\\'re describing the keys (possible subject #1) or the\\n        cabinet (possible subject #2). Having decided it is the keys, the second determination would be whether it is\\n        singular or plural.\\n\\n\\n\\n            The model is able to assign a higher ranking to is, which is the correct token. Every layer\\n            in the model managed to rank \" is\" higher than \" are\". The ranking of \" are\" remains high, however, as far as\\n            rankings are concerned (the delta in probability scores might indicate otherwise, however).\\n        \\n\\nContrast your answer for the first question with the following variation:\\nThe key to the cabinets ______ \\nThe figures in this section visualize the hidden-state evolution of the tokens \" is\" and \" are\". The numbers\\n        in the cells are their ranking in the position of the blank (Both columns address the same position in the\\n        sequence, they\\'re not subsequent positions as was the case in the previous visualization).\\nThe first figure (showing the rankings for the sequence \"The keys to the cabinet\") raises the question of why do five layers fail the task and only the final layer sets the record\\n        straight. This is likely a similar effect to that observed in BERT of the final layer being the most\\n        task-specific. It is also worth investigating whether that capability of succeeding at the task is predominantly localized in\\n        Layer 5, or if the Layer is only the final expression in a circuit\\n        spanning multiple layers which is especially sensitive to subject-verb agreement.\\n    \\nProbing for bias\\nThis method can shed light on questions of bias and where they might emerge in a model. The following figures, for example, probe for the model\\'s gender expectation associated with different professions:\\n\\n\\n\\n\\nFigure: Probing bias in the model\\'s association of gender with professions - Doctor and nurse\\n            The first five layers all rank \" man\" higher than \" woman\" for both professions. For the nursing profession, the final layer decisively elevates \" woman\" to a higher ranking than \" man\".\\n        \\n\\nMore systemaic and nuanced examination of bias in contextualized word embeddings (another term for the vectors we\\'ve been referring to as \"hidden states\") can be found in .\\n\\nYour turn!\\nYou can proceed to do your own experiments using Ecco and the three notebooks in this article:\\n\\n\\n\\nOutput Token Scores\\n\\n\\nEvolution of Selected Token\\n\\n\\nComparing Token Rankings\\n\\n\\n\\nYou can report issues you run into at the Ecco\\'s Github page. Feel free to share any interesting findings at the Ecco Discussion board. I invite you again to read Interpreting GPT the Logit Lens and see the various ways the author examines such a visualization. I leave you with a small gallery of examples showcasing the responses of different models to different input prompts.\\n\\nGallery\\n \\nInput: \"Heathrow airport is located in the city of\"\\nModel: DistilGPT2\\n                \\n\\n\\nInput: \"Some of the most glorious historical attractions in Spain date from the period of Muslim rule, including The Mezquita, built as the Great Mosque of Cordoba and the Medina Azahara, also in Cordoba and now in ruins but still visitable as such and built as the Madinat al-Zahra, the Palace of al-Andalus; and the Alhambra in Granada, a splendid, intact palace. There are also two synagogues still standing that were built during the era of Muslim Spain: Santa Maria la Blanca in Toledo and the Synagogue of Cordoba, in the Old City. Reconquista and Imperial era\"\\nModel: DistilGPT2\\n        \\n \\n\\nModel: GPT2-Large\\n\\n\\nModel: GPT2-XL\\n\\n\\nInput: \"The countires of the European Union are:\\\\n1. Austria\\\\n2. Belgium\\\\n3.\\n            Bulgaria\\\\n4.\" \\nModel: DistilGPT2\\n        \\n\\nModel: GPT2-Large\\n\\nModel: GPT2-XL\\n\\n\\nAcknowledgements\\nThis article was vastly improved thanks to feedback on earlier drafts provided by\\n        Abdullah Almaatouq,\\n        Anfal Alatawi,\\n        Fahd Alhazmi,\\n        Hadeel Al-Negheimish,\\n        Isabelle Augenstein,\\n        Jasmijn Bastings,\\n        Najwa Alghamdi,\\n        Pepa Atanasova, and\\n        Sebastian Gehrmann.\\n    \\nReferences\\n\\n\\nCitation\\n\\n\\nIf you found this work helpful for your research, please cite it as following:\\n\\n\\nAlammar, J. (2021). Finding the Words to Say: Hidden State Visualizations for Language Models [Blog post]. Retrieved from https://jalammar.github.io/hidden-states/\\n\\n\\n\\nBibTex:\\n\\n\\n@misc{alammar2021hiddenstates, \\n  title={Finding the Words to Say: Hidden State Visualizations for Language Models},\\n  author={Alammar, J},\\n  year={2021},\\n  url={https://jalammar.github.io/hidden-states/}\\n}\\n\\n\\n\\n\\n\\n\\n\\n    Written on January 19, 2021\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to get notified about upcoming posts by email\\n\\nEmail Address \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://jalammar.github.io/hidden-states/', 'title': 'Finding the Words to Say: Hidden State Visualizations for Language Models – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"\\n\\n\\n\\n\\n\\n\\n\\nBy visualizing the hidden state between a model's layers, we can get some clues as to the model's \", 'language': 'No language found.'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\"https://jalammar.github.io/hidden-states/\",\n",
    "                       bs_kwargs=dict(parse_only=bs4.Soup) )\n",
    "loader.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

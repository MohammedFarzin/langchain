{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./data\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine learning\\Generative AI\\Langchain\\langchain\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={\"device\":\"cpu\"}, encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01504208  0.01111973 -0.0143903  ...  0.02992566 -0.01091165\n",
      " -0.03430151]\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array(embeddings.embed_query(docs[0].page_content)))\n",
    "print(np.array(embeddings.embed_query(docs[0].page_content)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is positional encoding?\"\n",
    "relevant_documents = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n"
     ]
    }
   ],
   "source": [
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B35B3D6230>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACE_API_TOKEN\"] = os.getenv(\"HUGGINGFACE_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! add_to_git_credential is not default parameter.\n",
      "                    add_to_git_credential was transferred to model_kwargs.\n",
      "                    Please make sure that add_to_git_credential is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\FARZIN\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nPositional encoding is a technique used in deep learning models to provide the model with information about the position of the input data in the sequence. It is often used in models that process sequential data, such as recurrent neural networks (RNNs) and transformers.\\n\\nThe idea behind positional encoding is that the model should be able to learn the relationships between different positions in the sequence, without having to learn the position information itself. This allows the model to focus on learning the underlying patterns in the data, rather than memorizing the positions of the inputs.\\n\\nPositional encoding is typically implemented as a fixed, learnable vector that is added to each input vector in the sequence. The vector is designed to have specific patterns that correspond to different positions in the sequence. For example, in the case of sinusoidal positional encoding, the vector has components that are sinusoidal functions of the position.\\n\\nPositional encoding has been shown to be effective in improving the performance of deep learning models on sequential data tasks, such as language modeling and speech recognition. It allows the model to learn the relative positions of inputs in the sequence, even if the inputs themselves are shifted or transformed in some way.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint \n",
    "\n",
    "hf = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    temperature=0.4,\n",
    "    max_new_tokens=2048, \n",
    "    verbose=True, \n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_TOKEN\"),\n",
    "    add_to_git_credential=True)\n",
    "query = \"What is positional encoding?\"\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaa7001329f44d780d0c8853ae18139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine learning\\Generative AI\\Langchain\\langchain\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hugging face models run locally thorugh hugging face pipelines\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0.4, \"max_new_tokens\": 2048},\n",
    ")\n",
    "hf_pipeline.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following context to generate a response to the following question:\n",
    "Please try to answer to the question concisely and accurately.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(prompt_template, input_variable =[\"context,\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(llm=hf_pipeline, chain_type=\"stuff\", retriever=retriever, return_source_document=True, chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Differences in the uninsured rate by state in 2022\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = RetrievalQA.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
